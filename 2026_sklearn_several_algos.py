# -*- coding: utf-8 -*-
"""Untitled16.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rWGlsBl6L1g7120JVJyZAUcankCOE9J6
"""

from sklearn import datasets
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from matplotlib.colors import ListedColormap
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score, f1_score
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn import decomposition


######################################
## set parameters

np.set_printoptions(threshold=np.inf) ## print all values in numpy array

#######################################
## load problem data

#f_numpy = open("input_data_no_none.csv","rb")
#Matrix_data = np.loadtxt(f_numpy, delimiter=",", skiprows=1)
#print Matrix_data
## 1-7 are the features and 0 is the class
#X = Matrix_data[:, [1,2,3,4,5,6,7]] # for actual ml. cannot plot this
#X = Matrix_data[:, [1,2]] #just (for plotting only) 2d
#y = Matrix_data[:, 0] #this is correct
#print y

iris = datasets.load_iris()
X = iris.data[:, [1,2,3]]
y = iris.target

def print_stats_percentage_train_test(algorithm_name, y_test, y_pred):
     print("------------------------------------------------------")
     print("------------------------------------------------------")

     print("algorithm is: ", algorithm_name)

     print('Accuracy: %.2f' % accuracy_score(y_test,   y_pred) )

     confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)
     print("confusion matrix")
     print(confmat)
     print('Precision: %.3f' % precision_score(y_true=y_test, y_pred=y_pred, average='weighted'))
     print('Recall: %.3f' % recall_score(y_true=y_test, y_pred=y_pred, average='weighted'))
     print('F1-measure: %.3f' % f1_score(y_true=y_test, y_pred=y_pred, average='weighted'))

## knn

def knn_rc(X_train_std, y_train, X_test_std, y_test):
    from sklearn.neighbors import KNeighborsClassifier
    knn = KNeighborsClassifier(n_neighbors=5, p=2, metric='minkowski')
    knn.fit(X_train_std, y_train)
    y_pred = knn.predict(X_test_std)
    print_stats_percentage_train_test("knn", y_test, y_pred)
    ##print_stats_10_fold_crossvalidation("knn",knn, X_train_std, y_train )

    ## pca_X_train_std, pca_X_test_std = convert_to_pca(X_train_std, X_test_std)
    ## plot_2d_graph_model(knn,9500, 9502, pca_X_train_std, pca_X_test_std, y_train, y_test )

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)

#logistic_regression_rc(X_train_std, y_train, X_test_std, y_test)
#svm_rc(X_train_std, y_train, X_test_std, y_test)
##decision_trees_rc(X_train, y_train, X_test, y_test) #
#random_forest_rc(X_train, y_train, X_test, y_test) #
##knn_rc(X_train, y_train, X_test, y_test) #
multilayer_perceptron_rc(X_train, y_train, X_test, y_test)

def decision_trees_rc(X_train, y_train, X_test, y_test):
    from sklearn.tree import DecisionTreeClassifier
    tree = DecisionTreeClassifier(criterion='entropy',
                               max_depth=30, random_state=0)
    tree.fit(X_train, y_train)
    y_pred = tree.predict(X_test)
    print_stats_percentage_train_test("decision trees", y_test, y_pred)
    ##print_stats_10_fold_crossvalidation("decision trees",tree,X_train,y_train)

    from sklearn.tree import export_graphviz
    features_list = [ "f"+str(i) for i in range(149)]
    '''
    export_graphviz(tree, out_file='tree.dot',
                 feature_names=features_list)   #['time','a','b','sum','x','y','z'])
    '''
    ##pca_X_train_std, pca_X_test_std = convert_to_pca(X_train_std, X_test_std)
    ##plot_2d_graph_model(tree,9500, 12000, pca_X_train_std, pca_X_test_std, y_train, y_test )

def multilayer_perceptron_rc(X_train_std, y_train, X_test_std, y_test):
    #X= [[0.,0.], [1., 1.]]
    #y = [0, 1]
    from sklearn.neural_network import MLPClassifier
    # (20, ) means 1 hidden layer with 20 neurons
    # (20, 20) would mean 2 hidden layers with 20 neurons each
    clf = MLPClassifier(solver='lbfgs', alpha=1e-5,
                        hidden_layer_sizes=(20,20), random_state=1)
    clf.fit(X_train_std, y_train)
    y_pred = clf.predict(X_test_std)


    print_stats_percentage_train_test("multilayer perceptron", y_test, y_pred)





